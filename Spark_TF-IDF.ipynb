{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark-Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Assignment\n",
        "\n",
        "## Bhavana Balakrishna Rao\n",
        "### 1008221023"
      ],
      "metadata": {
        "id": "65ZxkbvgqUKJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN5bHBMbj5Ym",
        "outputId": "50fddd74-3b98-4d3a-f336-8aa1567afb39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=c274df8f2edf8abeda2a7213731ae8934de6f8696aa534f7d8d674d21f47db5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Collecting awscli\n",
            "  Downloading awscli-1.22.95-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
            "Collecting colorama<0.4.4,>=0.2.5\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting docutils<0.16,>=0.10\n",
            "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
            "\u001b[K     |████████████████████████████████| 547 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting botocore==1.24.40\n",
            "  Downloading botocore-1.24.40-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 44.2 MB/s \n",
            "\u001b[?25hCollecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.24.40->awscli) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.24.40->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython) (0.7.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.8\n",
            "    Uninstalling rsa-4.8:\n",
            "      Successfully uninstalled rsa-4.8\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.22.95 botocore-1.24.40 colorama-0.4.3 docutils-0.15.2 jmespath-1.0.0 rsa-4.7.2 s3transfer-0.5.2 urllib3-1.26.9\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install awscli ipython # Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "\n",
        "import collections\n",
        "import itertools\n",
        "import os\n",
        "import string"
      ],
      "metadata": {
        "id": "nw0f7ejqkiXC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"PySpark demo\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "0KUL02kHkiaO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload dataset_en_train.csv and dataset_en_test.csv."
      ],
      "metadata": {
        "id": "kLlA70Dy2KUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json('./dataset*.json')"
      ],
      "metadata": {
        "id": "CMTsYhrlnlOo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOmdeMZhlMjc",
        "outputId": "7273cecc-01cf-4bf0-fffa-96c35161c0f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------------+------------------+--------------------+----------+--------------------+-------------------+-----+\n",
            "|language|   product_category|        product_id|         review_body| review_id|        review_title|        reviewer_id|stars|\n",
            "+--------+-------------------+------------------+--------------------+----------+--------------------+-------------------+-----+\n",
            "|      en|          furniture|product_en_0740675|Arrived broken. M...|en_0964290|I'll spend twice ...|reviewer_en_0342986|    1|\n",
            "|      en|   home_improvement|product_en_0440378|the cabinet dot w...|en_0690095|        Not use able|reviewer_en_0133349|    1|\n",
            "|      en|               home|product_en_0399702|I received my fir...|en_0311558|The product is junk.|reviewer_en_0152034|    1|\n",
            "|      en|           wireless|product_en_0444063|This product is a...|en_0044972|Fucking waste of ...|reviewer_en_0656967|    1|\n",
            "|      en|                 pc|product_en_0139353|went through 3 in...|en_0784379|              bubble|reviewer_en_0757638|    1|\n",
            "|      en|industrial_supplies|product_en_0705898|Poor quality. The...|en_0420650|Poor quality. The...|reviewer_en_0155342|    1|\n",
            "|      en|               home|product_en_0041998|Ordered 2 they sh...|en_0206383|     Not reliable ☹️|reviewer_en_0005698|    1|\n",
            "|      en|            kitchen|product_en_0523280|Followed directio...|en_0638563|      Waste of money|reviewer_en_0363065|    1|\n",
            "|      en|            apparel|product_en_0737171|There is a terrib...|en_0331944|Picture Doesn’t R...|reviewer_en_0434580|    1|\n",
            "|      en|         automotive|product_en_0912236|Unless you have t...|en_0220290| Collapses on itself|reviewer_en_0514794|    1|\n",
            "|      en|            apparel|product_en_0724267|I am disappointed...|en_0210507|  Not what I ordered|reviewer_en_0031791|    1|\n",
            "|      en|             camera|product_en_0574186|It doesn’t work a...|en_0143009|               Cheap|reviewer_en_0626468|    1|\n",
            "|      en|           wireless|product_en_0729559|was not pleased a...|en_0016212|I would not recom...|reviewer_en_0403830|    1|\n",
            "|      en|    lawn_and_garden|product_en_0610204|Followed instruct...|en_0112269|            One Star|reviewer_en_0050452|    1|\n",
            "|      en|           wireless|product_en_0522866|Absolutely horrib...|en_0537874|   Smudgy, horrible!|reviewer_en_0737063|    1|\n",
            "|      en|           wireless|product_en_0628414|Does not play wit...|en_0056997|Does not play wit...|reviewer_en_0037773|    1|\n",
            "|      en|             camera|product_en_0250211|Doesn’t work and ...|en_0619473|         Ineffective|reviewer_en_0056679|    1|\n",
            "|      en|              watch|product_en_0566399|Beautiful way. Do...|en_0533035|Beautiful way. Do...|reviewer_en_0488191|    1|\n",
            "|      en|               home|product_en_0304984|Stems were broken...|en_0832890|   stems were broken|reviewer_en_0667005|    1|\n",
            "|      en|            apparel|product_en_0387159|Buy one size smal...|en_0550306|       Way too large|reviewer_en_0627216|    1|\n",
            "+--------+-------------------+------------------+--------------------+----------+--------------------+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRK1me6Xobu3",
        "outputId": "560de5e4-f4d4-417a-dfe7-26cbff6a5adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66258"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, countDistinct\n",
        "\n",
        "#df.agg(*(countDistinct(col(c)).alias(c) for c in df.columns))\n",
        "df.select(countDistinct(\"review_id\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwJ8oTLpUZBe",
        "outputId": "224d2fa2-fac7-40d1-d671-9d75313e822b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+\n",
            "|count(DISTINCT review_id)|\n",
            "+-------------------------+\n",
            "|                   179625|\n",
            "+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Size of the dataframe - (66,258, 8)"
      ],
      "metadata": {
        "id": "q-rpYuU9sw9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"rdd = spark.read.json('./dataset_en_train.json').rdd\n",
        "rdd.flatMap(lambda x: x.review_body.lower().split() + \\\n",
        "        x.review_title.lower().split()) \\\n",
        "   .groupBy(lambda x: x) \\\n",
        "   .map(lambda x: (x[0], len(x[1]))) \\\n",
        "   .sortBy(lambda x: x[1], ascending=False) \\\n",
        "   .take(100)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pRugcyQ5lMmK",
        "outputId": "7866b0e4-af22-41d2-896c-996583689bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"rdd = spark.read.json('./dataset_en_train.json').rdd\\nrdd.flatMap(lambda x: x.review_body.lower().split() +         x.review_title.lower().split())    .groupBy(lambda x: x)    .map(lambda x: (x[0], len(x[1])))    .sortBy(lambda x: x[1], ascending=False)    .take(100)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram(l, n):\n",
        "  nltk.download('stopwords', download_dir='/tmp/nltk')\n",
        "  nltk.data.path.append('/tmp/nltk')\n",
        "  translation_table = str.maketrans('', '', string.punctuation)\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  words = [w.translate(translation_table).lower() for w in l.split() if w.translate(translation_table).lower() not in stopwords]\n",
        "  d = collections.deque(maxlen=n)\n",
        "  d.extend(words[:n])\n",
        "  words = words[n:]\n",
        "  for window, word in zip(itertools.cycle((d,)), words):\n",
        "    yield(' '.join(window))\n",
        "    d.append(word)"
      ],
      "metadata": {
        "id": "LWj5TpUmlMo3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_grams(n):\n",
        "  def f(x):\n",
        "    return [Row(n=n, id=x.review_id, stars=x.stars, tokens=ng) for ng in list(ngram(x.review_title, n))] + \\\n",
        "           [Row(n=n, id=x.review_id, stars=x.stars, tokens=ng) for ng in list(ngram(x.review_body, n))]\n",
        "  return f"
      ],
      "metadata": {
        "id": "yLJsArpTlMrb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grams = df.rdd.flatMap(lambda x: split_grams(1)(x) + split_grams(2)(x) + split_grams(3)(x))\n",
        "#grams.toDF().registerTempTable(\"tokens\")\n",
        "\n",
        "# using 1-, 2- and 3-word sequences\n",
        "grams = df.rdd.flatMap(lambda x: split_grams(1)(x) + split_grams(2)(x) + split_grams(3)(x))\n",
        "grams.toDF().registerTempTable(\"table\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlfl2Pd1yLVo",
        "outputId": "12688094-f05f-4eba-f1bb-3fb70ee3cf67"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py:140: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  FutureWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" \n",
        "SELECT *\n",
        "FROM tokens\n",
        "WHERE n=2\n",
        "\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1qX_ibj6P9X",
        "outputId": "864abe50-302d-410a-a0da-5e08021b1ae6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----+-------------------+\n",
            "|n  |id        |stars|tokens             |\n",
            "+---+----------+-----+-------------------+\n",
            "|2  |en_0964290|1    |ill spend          |\n",
            "|2  |en_0964290|1    |spend twice        |\n",
            "|2  |en_0964290|1    |twice amount       |\n",
            "|2  |en_0964290|1    |amount time        |\n",
            "|2  |en_0964290|1    |time boxing        |\n",
            "|2  |en_0964290|1    |boxing whole       |\n",
            "|2  |en_0964290|1    |whole useless      |\n",
            "|2  |en_0964290|1    |useless thing      |\n",
            "|2  |en_0964290|1    |thing send         |\n",
            "|2  |en_0964290|1    |send back          |\n",
            "|2  |en_0964290|1    |back 1star         |\n",
            "|2  |en_0964290|1    |1star review       |\n",
            "|2  |en_0964290|1    |arrived broken     |\n",
            "|2  |en_0964290|1    |broken manufacturer|\n",
            "|2  |en_0964290|1    |manufacturer defect|\n",
            "|2  |en_0964290|1    |defect two         |\n",
            "|2  |en_0964290|1    |two legs           |\n",
            "|2  |en_0964290|1    |legs base          |\n",
            "|2  |en_0964290|1    |base completely    |\n",
            "|2  |en_0964290|1    |completely formed  |\n",
            "+---+----------+-----+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF for negative reviews"
      ],
      "metadata": {
        "id": "4RRVKTLSkQX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for n=1\n",
        "spark.sql(\"\"\" \n",
        "WITH t1 as (\n",
        "  SELECT id, \n",
        "    CASE WHEN stars = 1 OR stars = 2 THEN \"negative\" ELSE \"positive\" END AS sentiment,\n",
        "    tokens\n",
        "  FROM table\n",
        "  WHERE n=1 AND (stars = 1 OR stars = 2)\n",
        "),\n",
        "t2 as (\n",
        "  SELECT\n",
        "    id, sentiment, tokens,\n",
        "    COUNT(tokens) OVER (PARTITION BY id, tokens) as token_freq,\n",
        "    COUNT(*) OVER (PARTITION BY id) as total_tokens_in_doc\n",
        "  FROM t1\n",
        "),\n",
        "t3 as (\n",
        "  SELECT id, tokens, token_freq, total_tokens_in_doc,\n",
        "\tCAST(token_freq AS DECIMAL) / CAST(total_tokens_in_doc AS DECIMAL) as tf\n",
        "\tFROM t2\n",
        "),\n",
        "t4 AS(\n",
        "\tSELECT DISTINCT tokens, COUNT(DISTINCT id) AS docs_with_token -- no. of documents with the given keyword (denominator in idf)\n",
        "\tFROM t3\n",
        "\tGROUP BY tokens\n",
        "),\n",
        "t5 as (\n",
        "  SELECT t3.*, LOG((SELECT COUNT(DISTINCT id) FROM t3)/ t4.docs_with_token) AS idf\n",
        "\tFROM t3\n",
        "\tLEFT JOIN t4\n",
        "\tON t3.tokens = t4.tokens\n",
        ")\n",
        "SELECT *, tf*idf AS tf_idf\n",
        "FROM t5\n",
        "GROUP BY tokens, id, token_freq, total_tokens_in_doc, tf, idf\n",
        "ORDER BY tf_idf DESC\n",
        "\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-BBzoPuy5lx",
        "outputId": "ce2d2088-0b8d-4d29-bb1e-538e87e3cbb4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------+----------+-------------------+-------------+------------------+------------------+\n",
            "|id        |tokens         |token_freq|total_tokens_in_doc|tf           |idf               |tf_idf            |\n",
            "+----------+---------------+----------+-------------------+-------------+------------------+------------------+\n",
            "|en_0560040|junkdoesn’t    |1         |1                  |1.00000000000|11.313742551100862|11.313742551100862|\n",
            "|en_0034742|expectedbut    |1         |1                  |1.00000000000|11.313742551100862|11.313742551100862|\n",
            "|en_0716472|junkwaste      |1         |1                  |1.00000000000|11.313742551100862|11.313742551100862|\n",
            "|en_0997052|spears         |1         |1                  |1.00000000000|11.313742551100862|11.313742551100862|\n",
            "|en_0072464|antidepressants|1         |1                  |1.00000000000|11.313742551100862|11.313742551100862|\n",
            "|en_0249014|cheapbreaks    |1         |1                  |1.00000000000|11.313742551100862|11.313742551100862|\n",
            "|en_0697509|sai            |1         |1                  |1.00000000000|11.313742551100862|11.313742551100862|\n",
            "|en_0742111|breakrip       |1         |1                  |1.00000000000|11.313742551100862|11.313742551100862|\n",
            "|en_0046893|hasalot        |1         |1                  |1.00000000000|11.313742551100862|11.313742551100862|\n",
            "|en_0610011|expectedtoo    |1         |1                  |1.00000000000|10.620595370540917|10.620595370540917|\n",
            "|en_0542321|orthodontic    |2         |2                  |1.00000000000|10.620595370540917|10.620595370540917|\n",
            "|en_0817437|faintly        |1         |1                  |1.00000000000|10.620595370540917|10.620595370540917|\n",
            "|en_0009792|reaper         |2         |2                  |1.00000000000|10.620595370540917|10.620595370540917|\n",
            "|en_0808728|shihpoo        |1         |1                  |1.00000000000|10.620595370540917|10.620595370540917|\n",
            "|en_0059163|petty          |1         |1                  |1.00000000000|10.215130262432753|10.215130262432753|\n",
            "|en_0187534|hokey          |1         |1                  |1.00000000000|10.215130262432753|10.215130262432753|\n",
            "|en_0761510|suppress       |2         |2                  |1.00000000000|10.215130262432753|10.215130262432753|\n",
            "|en_0316162|origional      |1         |1                  |1.00000000000|10.215130262432753|10.215130262432753|\n",
            "|en_0718596|granular       |2         |2                  |1.00000000000|9.927448189980971 |9.927448189980971 |\n",
            "|en_0460259|dragonfly      |1         |1                  |1.00000000000|9.70430463866676  |9.70430463866676  |\n",
            "+----------+---------------+----------+-------------------+-------------+------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for n=2\n",
        "spark.sql(\"\"\" \n",
        "WITH t1 as (\n",
        "  SELECT id, \n",
        "    CASE WHEN stars = 1 OR stars = 2 THEN \"negative\" ELSE \"positive\" END AS sentiment,\n",
        "    tokens\n",
        "  FROM table\n",
        "  WHERE n=2 AND (stars = 1 OR stars = 2)\n",
        "),\n",
        "t2 as (\n",
        "  SELECT\n",
        "    id, sentiment, tokens,\n",
        "    COUNT(tokens) OVER (PARTITION BY id, tokens) as token_freq,\n",
        "    COUNT(*) OVER (PARTITION BY id) as total_tokens_in_doc\n",
        "  FROM t1\n",
        "),\n",
        "t3 as (\n",
        "  SELECT id, tokens, token_freq, total_tokens_in_doc,\n",
        "\tCAST(token_freq AS DECIMAL) / CAST(total_tokens_in_doc AS DECIMAL) as tf\n",
        "\tFROM t2\n",
        "),\n",
        "t4 AS(\n",
        "\tSELECT DISTINCT tokens, COUNT(DISTINCT id) AS docs_with_token -- no. of documents with the given keyword (denominator in idf)\n",
        "\tFROM t3\n",
        "\tGROUP BY tokens\n",
        "),\n",
        "t5 as (\n",
        "  SELECT t3.*, LOG((SELECT COUNT(DISTINCT id) FROM t3)/ t4.docs_with_token) AS idf\n",
        "\tFROM t3\n",
        "\tLEFT JOIN t4\n",
        "\tON t3.tokens = t4.tokens\n",
        ")\n",
        "SELECT *, tf*idf AS tf_idf\n",
        "FROM t5\n",
        "GROUP BY tokens, id, token_freq, total_tokens_in_doc, tf, idf\n",
        "ORDER BY tf_idf DESC\n",
        "\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXJI4c_CpAeN",
        "outputId": "32154612-4bbd-4368-d4cb-24a70dd0ecff"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------------------+----------+-------------------+-------------+------------------+------------------+\n",
            "|id        |tokens                     |token_freq|total_tokens_in_doc|tf           |idf               |tf_idf            |\n",
            "+----------+---------------------------+----------+-------------------+-------------+------------------+------------------+\n",
            "|en_0283526|arrived dusty              |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0689412|billed never               |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0907324|application start          |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0806063|advertised nit             |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0779167|0 stari                    |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0902170|advertiseddefinitely wooded|1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0787265|12 ladybugs                |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0238288|awkward fitting            |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0678894|2 dry                      |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0952887|babydoll hair              |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0757355|absolutely clean           |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0503253|backlit never              |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0510877|abuse light                |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0204479|bags tear                  |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0176162|applied twice              |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0265457|balloons small             |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0825890|absolutely stainless       |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0136286|barcode box                |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0776368|big show                   |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "|en_0535627|barely bass                |1         |1                  |1.00000000000|11.295900655914194|11.295900655914194|\n",
            "+----------+---------------------------+----------+-------------------+-------------+------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for n=3\n",
        "spark.sql(\"\"\" \n",
        "WITH t1 as (\n",
        "  SELECT id, \n",
        "    CASE WHEN stars = 1 OR stars = 2 THEN \"negative\" ELSE \"positive\" END AS sentiment,\n",
        "    tokens\n",
        "  FROM table\n",
        "  WHERE n=3 AND (stars = 1 OR stars = 2)\n",
        "),\n",
        "t2 as (\n",
        "  SELECT\n",
        "    id, sentiment, tokens,\n",
        "    COUNT(tokens) OVER (PARTITION BY id, tokens) as token_freq,\n",
        "    COUNT(*) OVER (PARTITION BY id) as total_tokens_in_doc\n",
        "  FROM t1\n",
        "),\n",
        "t3 as (\n",
        "  SELECT id, tokens, token_freq, total_tokens_in_doc,\n",
        "\tCAST(token_freq AS DECIMAL) / CAST(total_tokens_in_doc AS DECIMAL) as tf\n",
        "\tFROM t2\n",
        "),\n",
        "t4 AS(\n",
        "\tSELECT DISTINCT tokens, COUNT(DISTINCT id) AS docs_with_token -- no. of documents with the given keyword (denominator in idf)\n",
        "\tFROM t3\n",
        "\tGROUP BY tokens\n",
        "),\n",
        "t5 as (\n",
        "  SELECT t3.*, LOG((SELECT COUNT(DISTINCT id) FROM t3)/ t4.docs_with_token) AS idf\n",
        "\tFROM t3\n",
        "\tLEFT JOIN t4\n",
        "\tON t3.tokens = t4.tokens\n",
        ")\n",
        "SELECT *, tf*idf AS tf_idf\n",
        "FROM t5\n",
        "GROUP BY tokens, id, token_freq, total_tokens_in_doc, tf, idf\n",
        "ORDER BY tf_idf DESC\n",
        "\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOCrexbppB-_",
        "outputId": "e6883574-15fe-4566-9ee8-90fbd1a36f89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------+-------------------+-------------+------------------+------------------+\n",
            "|id        |tokens              |token_freq|total_tokens_in_doc|tf           |idf               |tf_idf            |\n",
            "+----------+--------------------+----------+-------------------+-------------+------------------+------------------+\n",
            "|en_0786258|1200 lm like        |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0727514|6 days get          |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0155072|2 years starting    |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0481820|6 cans description  |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0967461|1 3 gave            |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0693305|3 month already     |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0579305|1 got two           |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0334354|3 pieces one        |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0442384|100 rayon thin      |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0298264|3 weeks yet         |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0087006|2 jars came         |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0002019|3 worked 1          |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0908374|2 one little        |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0400350|3040 cracked crushed|1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0256238|6 12 lights         |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0513631|4 failed 2          |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0761924|10 days deliver     |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0272992|500 lb capacity     |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0332914|2 months strap      |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "|en_0346246|5’ 9” shorter       |1         |1                  |1.00000000000|11.252261753742095|11.252261753742095|\n",
            "+----------+--------------------+----------+-------------------+-------------+------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DG98XWKHpAg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion of Results\n",
        "\n",
        "The TF-IDF for 1-, 2-, 3- n-grams were calculated. Each review is considered as a document and each n-gram is considered as a term. The results above shows the different n-grams that best represent negative reviews. Some of these phrases include 'arrived dusty', 'failed', '3 weeks yet'.\n",
        "\n",
        "The top 10% from the above results can be chosen as blacklisted keywords to identify negative reviews which can ultimately lead to increased customer satisfaction."
      ],
      "metadata": {
        "id": "tLRQJYqZRZ8-"
      }
    }
  ]
}